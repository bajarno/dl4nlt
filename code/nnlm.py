import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from encoders import BOWEncoder, ConvEncoder
from dataloader import get_dataloaders

class NNLM(nn.Module):
    """
    Implementation of NNLM (Bengio 2003)
    """
    def __init__(self, seq_len, vocab_size, embedding_dim, hidden_size, output_size, activation=torch.tanh):
        super(NNLM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.activation = activation
        self.order = seq_len
        self.vocab_size = vocab_size

        # hidden layers
        # hidden_size is either integer (for one hidden layer) or list of integers (for multiple).
        emb_cat_size = embedding_dim * seq_len
        if isinstance(hidden_size, int):
            hidden_size = [hidden_size]
 
        self.fc_hidden = []
        in_features = emb_cat_size
        for i, hs in enumerate(hidden_size):
            l = nn.Linear(in_features, hs)
            self.fc_hidden.append(l)
            self.add_module('fc{}'.format(i), l)
            in_features = hs

        # output layer
        self.fc_out = nn.Linear(hidden_size[-1], output_size)

    
    def forward(self, x):
        x = self.embedding(x)
        if x.dim() == 4:
            x = torch.reshape(x, (x.size(0), x.size(1), -1))
        else:
            x = torch.reshape(x, (x.size(0), -1))
        for fc in self.fc_hidden:
            x = self.activation(fc(x))
        out = self.fc_out(x) 
        return out


class FBModel(nn.Module):
    def __init__(self, encoder, nnlm):
        super(FBModel, self).__init__()
        self.encoder = encoder
        self.nnlm = nnlm

    def forward(self, x, y_c, output_length=None, teacher_forcing=True):
        """
        x: LongTensor [batch_size, x_seqlength]
        y_c: LongTensor:
             if not teacher_forcing: [batch_size, 1, nnlm.order]
             if teacher_forcing: [batch_size, output_length, nnlm.order]
        output_length: length of output to generate, None if  teacher_forcing
        teacher_forcing: if true, feed ngrams to nnlm generated by true output.
                         else, generate ngrams based on previous predictions.
        """

        batch_size = y_c.size(0)
        if teacher_forcing:
            if output_length is not None:
                raise RuntimeError("If teacher forcing is enabled, output_length is given by y_c.")
            output_length = y_c.size(1)

            # Generate context vectors from y_c
            nnlm_out = self.nnlm(y_c)

            # Generate document vectors from x
            enc_out = self.encoder(x, y_c)
            out = enc_out + nnlm_out

            return out

        else:
            # No teacher forcing, feed previous prediction
            if output_length is None:
                raise RuntimeError("If teacher forcing is enabled, output_length must be given.")
            raise NotImplementedError("no teacherforcing not implemented.")

    @property
    def num_params(self):
        model_parameters = filter(lambda p: p.requires_grad, model.parameters())
        return sum([np.prod(p.size()) for p in model_parameters])
            
def add_start_end(Y, n_start_tokens=1):
    b = Y.size(0)
    start = torch.full([b, n_start_tokens], Y[0,0], dtype=torch.long).to(Y.device)
    return torch.cat([start, Y], dim=1)

if __name__=="__main__":
    PAD_TOKEN = 0
    DEVICE = 'cuda'
    batch_size = 64
    train_loader, test_loader = get_dataloaders('../data/kaggle_parsed_preprocessed_5000_vocab.csv', batch_size=batch_size)
    vocab_size = len(train_loader.dataset.w2i) + 1
    embedding_dim = 128
    hidden_size = 128
    order = 3
    num_epochs = 10


    # define model
    encoder = BOWEncoder(vocab_size, embedding_dim, vocab_size)
    # encoder = ConvEncoder(vocab_size, embedding_dim, 4, hidden_size, vocab_size)
    nnlm = NNLM(order, vocab_size, embedding_dim, [hidden_size]*3, vocab_size)
    model = FBModel(encoder, nnlm).to(DEVICE)
    print('model params', model.num_params)
    opt = torch.optim.Adam(model.parameters())
    crit = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)

    for _ in range(num_epochs):
        for batch_idx, (Y, X) in enumerate(train_loader):
            # print(batch_idx, X.shape, Y.shape)
            
            Y = Y.to(DEVICE)
            Y = add_start_end(Y, order-1)
            X = X.to(DEVICE)
            # Make ngrams and targets
            y_c = torch.stack([Y[:, i:i+order] for i in range(0, Y.size(1)-order)], 1)
            y_t = Y[:, order:]
            # print('y_c', y_c.size())
            # print('y_t', y_t.size())


            opt.zero_grad()
            # forward pass
            out = model(X, y_c)
            # print('output', out.size())

            # Calculate loss
            loss = crit(out.transpose(2, 1), y_t)

            loss.backward()
            opt.step()

            if not batch_idx%20:
                print(batch_idx, 'loss', loss.item())
        