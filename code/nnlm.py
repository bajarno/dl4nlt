import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from encoders import BOWEncoder, ConvEncoder
from dataloader import get_dataloaders

class NNLM(nn.Module):
    """
    Implementation of NNLM (Bengio 2003)
    """
    def __init__(self, seq_len, vocab_size, embedding_dim, hidden_size, output_size, activation=torch.tanh):
        super(NNLM, self).__init__()
        self.activation = activation
        self.order = seq_len
        self.vocab_size = vocab_size

        # hidden layers
        # hidden_size is either integer (for one hidden layer) or list of integers (for multiple).
        emb_cat_size = embedding_dim * seq_len
        if isinstance(hidden_size, int):
            hidden_size = [hidden_size]
 
        self.fc_hidden = []
        in_features = emb_cat_size
        for i, hs in enumerate(hidden_size):
            l = nn.Linear(in_features, hs)
            self.fc_hidden.append(l)
            self.add_module('fc{}'.format(i), l)
            in_features = hs

        # output layer
        self.fc_out = nn.Linear(hidden_size[-1], output_size)

    def forward(self, x):
        if x.dim() == 4:
            x = torch.reshape(x, (x.size(0), x.size(1), -1))
        else:
            x = torch.reshape(x, (x.size(0), -1))
        for fc in self.fc_hidden:
            x = self.activation(fc(x))
        out = self.fc_out(x) 
        return out


class FBModel(nn.Module):
    def __init__(self, embedding, encoder, nnlm):
        super(FBModel, self).__init__()
        self.embedding = embedding
        self.encoder = encoder
        self.nnlm = nnlm

    def forward(self, x, y_c, xlen, ylen, output_length=None, teacher_forcing=True):
        """
        x: LongTensor [batch_size, x_seqlength]
        y_c: LongTensor:
             if not teacher_forcing: [batch_size, 1, nnlm.order]
             if teacher_forcing: [batch_size, output_length, nnlm.order]
        output_length: length of output to generate, None if  teacher_forcing
        teacher_forcing: if true, feed ngrams to nnlm generated by true output.
                         else, generate ngrams based on previous predictions.
        """

        if teacher_forcing:
            if output_length is not None:
                raise RuntimeError("If teacher forcing is enabled, output_length is given by y_c.")
            output_length = y_c.size(1)

            x_emb = self.embedding(x)
            y_c_emb = self.embedding(y_c)

            # Generate context vectors from y_c
            nnlm_out = self.nnlm(y_c_emb)

            # Generate document vectors from x
            enc_out = self.encoder(x_emb, y_c_emb, xlen, ylen)
            out = enc_out + nnlm_out

            return out

        else:
            # No teacher forcing, feed previous prediction
            if output_length is None:
                raise RuntimeError("If teacher forcing is enabled, output_length must be given.")

            outputs = []
            x_emb = self.embedding(x)
            for t in range(output_length):
                y_c_emb = self.embedding(y_c)
                nnlm_out = self.nnlm(y_c_emb)
                # Without teacher forcing, length of y is always one.
                ylen = torch.ones_like(ylen).to(ylen.device)
                enc_out = self.encoder(x_emb, y_c_emb, xlen, ylen)
                out = nnlm_out + enc_out
                outputs.append(out)

                # Make new ngram y_c with previous prediction
                pred = out.detach().argmax(-1)
                y_c = torch.cat([y_c[:, :, 1:], pred.unsqueeze(1)], dim=-1)

            out = torch.cat(outputs, dim=1)
            return out

    @property
    def num_params(self):
        model_parameters = filter(lambda p: p.requires_grad, model.parameters())
        return sum([np.prod(p.size()) for p in model_parameters])