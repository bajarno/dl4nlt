import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from encoders import BOWEncoder, ConvEncoder
from dataloader import get_dataloaders

class NNLM(nn.Module):
    """
    Implementation of NNLM (Bengio 2003)
    """
    def __init__(self, seq_len, vocab_size, embedding_dim, hidden_size, output_size, activation=torch.tanh):
        super(NNLM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.activation = activation
        self.order = seq_len
        self.vocab_size = vocab_size

        # hidden layers
        # hidden_size is either integer (for one hidden layer) or list of integers (for multiple).
        emb_cat_size = embedding_dim * seq_len
        if isinstance(hidden_size, int):
            hidden_size = [hidden_size]
 
        self.fc_hidden = []
        in_features = emb_cat_size
        for i, hs in enumerate(hidden_size):
            l = nn.Linear(in_features, hs)
            self.fc_hidden.append(l)
            self.add_module('fc{}'.format(i), l)
            in_features = hs

        # output layer
        self.fc_out = nn.Linear(hidden_size[-1], output_size)

    
    def forward(self, x):
        # embed [batch_size, seq_len] -> [batch_size, seq_len, embedding_dim]
        x = self.embedding(x)
        # cat embeddings [batch_size, seq_len, embedding_dim] -> [batch_size, seq_len * embedding_dim]
        x = torch.reshape(x, (x.size(0), -1))
        # hidden layers [b, sl * emb] -> [b, hidden]
        for fc in self.fc_hidden:
            x = self.activation(fc(x))
        # [b, hidden] -> [b, out]
        out = self.fc_out(x) 
        return out


class FBModel(nn.Module):
    def __init__(self, encoder, nnlm):
        super(FBModel, self).__init__()
        self.encoder = encoder
        self.nnlm = nnlm

    def forward(self, x, y_c, output_length=None, teacher_forcing=True):
        """
        x: LongTensor [batch_size, x_seqlength]
        y_c: LongTensor:
             if not teacher_forcing: [batch_size, 1, nnlm.order]
             if teacher_forcing: [batch_size, output_length, nnlm.order]
        output_length: length of output to generate, None if  teacher_forcing
        teacher_forcing: if true, feed ngrams to nnlm generated by true output.
                         else, generate ngrams based on previous predictions.
        """

        batch_size = y_c.size(0)
        if teacher_forcing:
            if output_length is not None:
                raise RuntimeError("If teacher forcing is enabled, output_length is given by y_c.")
            output_length = y_c.size(1)

            # Foreach ngram, target: encoder x, encode y_c, generate output
            outputs = torch.zeros(batch_size, output_length, self.nnlm.vocab_size).to(X.device)
            for i in range(output_length):
                enc_i = self.encoder(x, y_c[:, i])
                nnlm_i = self.nnlm(y_c[:, i])
                outputs[:, i, :] = enc_i + nnlm_i
            return outputs

        else:
            # No teacher forcing, feed previous prediction
            if output_length is None:
                raise RuntimeError("If teacher forcing is enabled, output_length must be given.")
            raise NotImplementedError("no teacherforcing not implemented.")

    @property
    def num_params(self):
        model_parameters = filter(lambda p: p.requires_grad, model.parameters())
        return sum([np.prod(p.size()) for p in model_parameters])
            
def add_start_end(Y, start_token, end_token, n_start_tokens=1):
    b = Y.size(0)
    start = torch.full([b, n_start_tokens], start_token, dtype=torch.long).to(Y.device)
    end = torch.full([b, 1], end_token, dtype=torch.long).to(Y.device)
    return torch.cat([start, Y, end], dim=1)

if __name__=="__main__":
    # torch.manual_seed(1)
    # # constants
    PAD_TOKEN = 0
    # START_TOKEN = 1
    # EOS_TOKEN = 2
    DEVICE = 'cpu'

    # hyperparams


    # # define model
    # # encoder = BOWEncoder(vocab_size, embedding_dim, vocab_size)
    # encoder = ConvEncoder(vocab_size, embedding_dim, 4, hidden_size, vocab_size)
    # nnlm = NNLM(order, vocab_size, embedding_dim, [hidden_size]*3, vocab_size)
    # model = FBModel(encoder, nnlm).to(DEVICE)
    # print('model params', model.num_params)

    # # data
    # batch_size = 4
    # seqlength_x = 100
    # seqlength_y = 8
    # X = torch.LongTensor(batch_size, seqlength_x).random_(3, vocab_size).to(DEVICE)
    # Y = torch.LongTensor(batch_size, seqlength_y).random_(3, vocab_size).to(DEVICE)
    # print('X', X.size())
    # print('Y', Y.size())



    # # forward pass
    # out = model(X, y_c)
    # print('output', out.size())

    # # Calculate loss
    # crit = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)
    # loss = crit(out.transpose(2, 1), y_t)
    # print('loss', loss.item(), 'on', loss.device)
    train_loader, test_loader = get_dataloaders('test/preprocessed_testdata.csv')

    vocab_size = len(train_loader.dataset.w2i) + 1
    embedding_dim = 128
    hidden_size = 128
    order = 1


    # define model
    # encoder = BOWEncoder(vocab_size, embedding_dim, vocab_size)
    encoder = ConvEncoder(vocab_size, embedding_dim, 4, hidden_size, vocab_size)
    nnlm = NNLM(order, vocab_size, embedding_dim, [hidden_size]*3, vocab_size)
    model = FBModel(encoder, nnlm).to(DEVICE)
    print('model params', model.num_params)
    opt = torch.optim.Adam(model.parameters())

    crit = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)

    for i in range(100):
        for batch_idx, (X, Y) in enumerate(train_loader):
            # print(batch_idx, X.shape, Y.shape)
            
            # Make ngrams and targets
            y_c = torch.stack([Y[:, i:i+order] for i in range(0, Y.size(1)-order)], 1)
            y_t = Y[:, order:]
            # print('y_c', y_c.size())
            # print('y_t', y_t.size())


            opt.zero_grad()
            # forward pass
            out = model(X, y_c)
            # print('output', out.size())

            # Calculate loss
            loss = crit(out.transpose(2, 1), y_t)
            print('loss', loss.item(), 'on', loss.device)

            loss.backward()
            opt.step()