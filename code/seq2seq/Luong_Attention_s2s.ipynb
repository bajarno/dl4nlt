{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Luong general global attention\n",
    "# https://arxiv.org/pdf/1508.04025.pdf\n",
    "#\n",
    "# score(d_i, e_j) = dot(d_i, W_a @ e_j)\n",
    "# Where d_i is i-th decoder output, e_j is j-th encoder output\n",
    "# a_(d_i -> e_j) = softmax(score(d_i, e_j))\n",
    "#\n",
    "# Implementation differs slightly from paper, W_a is nn.Linear (has bias parameters)\n",
    "# Luong dot attention: remove W_a\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "encoder_outputs = 4\n",
    "decoder_outputs = 1\n",
    "hidden_size = 32\n",
    "\n",
    "# all shapes are batch-first.\n",
    "encoder_outputs = torch.FloatTensor(batch_size, encoder_outputs, hidden_size).random_(-1, 1)\n",
    "hidden = torch.FloatTensor(batch_size, decoder_outputs, hidden_size).random_(-1, 1)\n",
    "\n",
    "# attention weights\n",
    "attn = nn.Linear(hidden_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manual calculation, sanity check\n",
    "\n",
    "batch_size = encoder_outputs.size(0)\n",
    "max_len = encoder_outputs.size(1)\n",
    "decoder_outputs = hidden.size(1)\n",
    "\n",
    "# Create variable to store attention energies\n",
    "attn_weights = torch.zeros(batch_size, decoder_outputs, max_len)\n",
    "\n",
    "# For each batch of encoder outputs\n",
    "for b in range(batch_size):\n",
    "    # Calculate energy for each encoder output\n",
    "    for i in range(max_len):\n",
    "        for j in range(decoder_outputs):\n",
    "            attn_weights[b, j, i] = hidden[b, j].dot(attn(encoder_outputs[b, i]))\n",
    "  \n",
    "attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "aw_manual = attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With bmm\n",
    "# Also works with multiple decoder outputs (hidden = [batch_size, n_outputs, hidden_size])\n",
    "# Useful for training if teacher_forcing.\n",
    "aw_batch = torch.bmm(hidden, attn(encoder_outputs).permute(0,2,1))\n",
    "aw_batch = torch.softmax(aw_batch, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn size torch.Size([2, 1, 4])\n",
      "allclose = True\n",
      "tensor([[[0.5040, 0.2746, 0.1142, 0.1071]],\n",
      "\n",
      "        [[0.1295, 0.1472, 0.6274, 0.0959]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[0.5040, 0.2746, 0.1142, 0.1071]],\n",
      "\n",
      "        [[0.1295, 0.1472, 0.6274, 0.0959]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Check manual vs bmm\n",
    "print('attn size', aw_batch.size())\n",
    "print('allclose =', torch.allclose(aw_manual, aw_batch))\n",
    "print(aw_manual)\n",
    "print(aw_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask\n",
      "tensor([[[1., 1., 1.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0.],\n",
      "         [1., 1., 0.],\n",
      "         [1., 1., 0.],\n",
      "         [1., 1., 0.]]])\n",
      "logits\n",
      "tensor([[[ 0.9804, -0.0003,  0.7664],\n",
      "         [ 0.6130,  0.7238, -0.9923],\n",
      "         [ 0.8235, -0.8525,  0.8965],\n",
      "         [-0.0414, -0.8862,  0.2781]],\n",
      "\n",
      "        [[-0.2035, -0.8982, -0.3625],\n",
      "         [ 0.1031,  0.7788, -0.3074],\n",
      "         [ 0.5940,  0.1252,  0.0337],\n",
      "         [ 0.1913, -0.6563, -0.8312]]])\n",
      "masked softmax\n",
      "tensor([[[0.4582, 0.1719, 0.3699],\n",
      "         [0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "        [[0.6670, 0.3330, 0.0000],\n",
      "         [0.3372, 0.6628, 0.0000],\n",
      "         [0.6151, 0.3849, 0.0000],\n",
      "         [0.7001, 0.2999, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# MASK\n",
    "\n",
    "def lengths_to_mask_torch(lengths, max_length=None):\n",
    "    if max_length is None:\n",
    "        max_length = lengths.data.max()\n",
    "    batch_size = lengths.size(0)\n",
    "        \n",
    "    rng = torch.arange(max_length).expand(batch_size, max_length)\n",
    "    lengths = lengths.unsqueeze(1).expand(batch_size, max_length)\n",
    "    return rng < lengths\n",
    "\n",
    "def make_2d_mask(l1, l2):\n",
    "    \"\"\"\n",
    "    Generates 2d binary mask by taking the carthesian product of mask(l1) x mask(l2)\n",
    "    example:\n",
    "    \n",
    "    make_2d_mask(torch.LongTensor([3,2]), torch.LongTensor([1,4]))\n",
    "    \n",
    "    tensor([[[1., 0., 0., 0.],\n",
    "             [1., 0., 0., 0.],\n",
    "             [1., 0., 0., 0.]],\n",
    "\n",
    "            [[1., 1., 1., 1.],\n",
    "             [1., 1., 1., 1.],\n",
    "             [0., 0., 0., 0.]]])\n",
    "    \"\"\"\n",
    "    m1 = lengths_to_mask_torch(l1)\n",
    "    m2 = lengths_to_mask_torch(l2)\n",
    "    return torch.bmm(m1.unsqueeze(2), m2.unsqueeze(1)).float()\n",
    "\n",
    "def binary_mask_to_softmax(mask, zero_val=-1e10):\n",
    "    smask = torch.zeros_like(mask)\n",
    "    smask[mask == 0] = zero_val\n",
    "    return smask.float()\n",
    "\n",
    "def masked_softmax(logits, mask, dim):\n",
    "    mask = binary_mask_to_softmax(mask).to(logits.device)\n",
    "    return torch.softmax(logits + mask, dim)\n",
    "    \n",
    "# TEST\n",
    "encoder_lengths = torch.LongTensor([3,2])\n",
    "decoder_lengths = torch.LongTensor([1,4])\n",
    "m = make_2d_mask(decoder_lengths, encoder_lengths)\n",
    "logits = torch.rand_like(m) * 2 - 1\n",
    "\n",
    "print('mask')\n",
    "print(m)\n",
    "print('logits')\n",
    "print(logits)\n",
    "print('masked softmax')\n",
    "print(masked_softmax(logits, m, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MaskedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Luong global attention with 'general' score mechanism.\n",
    "    https://arxiv.org/pdf/1508.04025.pdf\n",
    "    \n",
    "    score(d_i, e_j) = dot(d_i, W_a @ e_j)\n",
    "    Where d_i is i-th decoder output, e_j is j-th encoder output\n",
    "    a_(d_i -> e_j) = softmax(score(d_i, e_j))\n",
    "    \n",
    "    Implementation differs slightly from paper, W_a is nn.Linear (bias parameters)\n",
    "    For 'dot' score mechanism: remove W\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super(MaskedAttention, self).__init__()\n",
    "        self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, encoder_outputs, decoder_outputs, encoder_lengths=None, decoder_lengths=None):\n",
    "        \"\"\"\n",
    "        encoder outputs: FloatTensor shape (batch_size, encoder_length, hidden_size)\n",
    "        decoder_outputs: FloatTensor shape (batch_size, decoder_length, hidden_size)\n",
    "        encoder_lengths: LongTensor shape (batch_size)\n",
    "        decoder_lengths: LongTensor shape (batch_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # score[b, i, j] = score for sample b, decoder_i -> encoder_j \n",
    "        scores = torch.bmm(decoder_outputs, self.W(encoder_outputs).permute(0,2,1))\n",
    "        \n",
    "        if encoder_lengths is None:\n",
    "            a = torch.softmax(scores, -1)\n",
    "        else:\n",
    "            mask = make_2d_mask(decoder_lengths, encoder_lengths)\n",
    "            a = masked_softmax(scores, mask, -1)\n",
    "        return a\n",
    "        \n",
    "        \n",
    "def lengths_to_mask(lengths, max_length=None):\n",
    "    \"\"\"\n",
    "    creates mask m for each length in length, \n",
    "    where m[i, :lengths[i]] == 1 else 0\n",
    "    \"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = lengths.data.max()\n",
    "    batch_size = lengths.size(0)\n",
    "\n",
    "    rng = torch.arange(max_length).expand(batch_size, max_length)\n",
    "    lengths = lengths.unsqueeze(1).expand(batch_size, max_length)\n",
    "    return rng < lengths\n",
    "\n",
    "def make_2d_mask(l1, l2):\n",
    "    \"\"\"\n",
    "    Generates 2d binary mask by taking the carthesian product of mask(l1) x mask(l2)\n",
    "    example:\n",
    "    \n",
    "    >>> make_2d_mask(torch.LongTensor([3,2]), torch.LongTensor([1,4]))\n",
    "    tensor([[[1., 0., 0., 0.],\n",
    "             [1., 0., 0., 0.],\n",
    "             [1., 0., 0., 0.]],\n",
    "\n",
    "            [[1., 1., 1., 1.],\n",
    "             [1., 1., 1., 1.],\n",
    "             [0., 0., 0., 0.]]])\n",
    "    \"\"\"\n",
    "    m1 = lengths_to_mask_torch(l1)\n",
    "    m2 = lengths_to_mask_torch(l2)\n",
    "    return torch.bmm(m1.unsqueeze(2), m2.unsqueeze(1)).float()\n",
    "\n",
    "        \n",
    "\n",
    "def masked_softmax(logits, mask, dim, minval=-1e10):\n",
    "    smask = torch.zeros_like(mask)\n",
    "    smask[mask == 0] = minval\n",
    "    return torch.softmax(logits + smask.to(logits.device), dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without mask\n",
      "tensor([[[0.5040, 0.2746, 0.1142, 0.1071]],\n",
      "\n",
      "        [[0.1295, 0.1472, 0.6274, 0.0959]]], grad_fn=<SoftmaxBackward>)\n",
      "allclose True\n",
      "with mask\n",
      "tensor([[[0.5040, 0.2746, 0.1142, 0.1071]],\n",
      "\n",
      "        [[0.4679, 0.5321, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "from attnseq2seq import MaskedAttention\n",
    "\n",
    "# test with previous values\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "encoder_outputs = 4\n",
    "decoder_outputs = 1\n",
    "hidden_size = 32\n",
    "\n",
    "encoder_outputs = torch.FloatTensor(batch_size, encoder_outputs, hidden_size).random_(-1, 1)\n",
    "hidden = torch.FloatTensor(batch_size, decoder_outputs, hidden_size).random_(-1, 1)\n",
    "\n",
    "attn = MaskedAttention(hidden_size)\n",
    "\n",
    "# NO MASK\n",
    "aw_final = attn.forward(encoder_outputs, hidden)\n",
    "print('Without mask')\n",
    "print(aw_final)\n",
    "print('allclose', torch.allclose(aw_final, aw_batch))\n",
    "\n",
    "# WITH MASK\n",
    "encoder_lengths = torch.LongTensor([4,2])\n",
    "decoder_lengths = torch.LongTensor([1,1])\n",
    "aw_masked = attn.forward(encoder_outputs, hidden, encoder_lengths, decoder_lengths)\n",
    "print('with mask')\n",
    "print(aw_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "encoder_outputs = 4\n",
    "decoder_outputs = 3\n",
    "hidden_size = 32\n",
    "\n",
    "encoder_outputs = torch.FloatTensor(batch_size, encoder_outputs, hidden_size).random_(-1, 1)\n",
    "hidden = torch.FloatTensor(batch_size, decoder_outputs, hidden_size).random_(-1, 1)\n",
    "encoder_lengths = torch.LongTensor([4,2])\n",
    "decoder_lengths = torch.LongTensor([1,1])\n",
    "aw_masked = attn.forward(encoder_outputs, hidden, encoder_lengths, decoder_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 4, 32])\n",
      "torch.Size([2, 3, 32]) torch.Size([2, 3, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_weights, encoder_outputs -> context vectors\n",
    "# Weighted average of encoder outputs, weights are attention weights\n",
    "# MANUAL\n",
    "ctx = torch.zeros(batch_size, decoder_outputs, hidden_size)\n",
    "# for each sample\n",
    "for b in range(batch_size):\n",
    "    # for each decoder output\n",
    "    for i in range(decoder_outputs):\n",
    "        # c[b, i] = Sum_j aw_[b, i, j] * encoder_outputs[b, j]\n",
    "        for j in range(4):\n",
    "            ctx[b, i] += aw_masked[b, i, j] * encoder_outputs[b, j]\n",
    "\n",
    "# BMM\n",
    "print(aw_masked.size(), encoder_outputs.size())\n",
    "ctx_b = torch.bmm(aw_masked, encoder_outputs)\n",
    "\n",
    "# Check\n",
    "print(ctx.size(), ctx_b.size())\n",
    "torch.allclose(ctx, ctx_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6, 12,  4,  8,  1, 14,  6,  0,  0,  0],\n",
       "         [ 9, 19, 12,  5,  9, 19, 18,  8,  0,  0],\n",
       "         [11, 13,  5, 16, 19,  5,  5, 12,  9, 13]]),\n",
       " tensor([[ 4,  6,  6,  8, 12, 14,  1],\n",
       "         [ 8, 12, 18,  1,  0,  0,  0],\n",
       "         [12, 16,  1,  0,  0,  0,  0]]),\n",
       " tensor([ 7,  8, 10]),\n",
       " tensor([7, 4, 3]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dummy_batch(batch_size, vocab_size, sequence_length):\n",
    "    \"\"\"\n",
    "    task: sort list of numbers, remove odd numbers, end with 1 as EOS.\n",
    "    \"\"\"\n",
    "    lengths = np.random.randint(sequence_length//2, sequence_length, batch_size)\n",
    "    lengths[0] = sequence_length\n",
    "    X = [np.random.randint(1, vocab_size, l) for l in lengths]\n",
    "    Y = [np.array([i for i in np.sort(x) if i % 2 == 0] + [1]) for x in X]\n",
    "    Y_lengths = np.array([len(y) for y in Y])\n",
    "    Y_max = np.max([len(y) for y in Y])\n",
    "    X = [np.pad(x, (0, sequence_length-l), mode='constant') for x, l in zip(X, lengths)]\n",
    "    X = np.vstack(X)\n",
    "    Y = np.vstack([np.pad(y, (0, Y_max-len(y)), mode='constant') for y in Y])\n",
    "    \n",
    "    idx = Y_lengths.argsort(axis=0)[::-1]\n",
    "    X = X[idx]\n",
    "    Y = Y[idx]\n",
    "    lengths = lengths[idx]\n",
    "    Y_lengths = Y_lengths[idx]\n",
    "    return (torch.LongTensor(X), \n",
    "            torch.LongTensor(Y),\n",
    "            torch.LongTensor(lengths),\n",
    "            torch.LongTensor(Y_lengths))\n",
    "\n",
    "dummy_batch(3, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from attnseq2seq import S2SEncoder, S2SAttnDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def s2s(encoder, decoder, X, Y, xl, yl, device):\n",
    "    # Shift input one to right, remove EOS token\n",
    "    Y_in = Y.clone()\n",
    "    Y_in[Y_in==1] = 0\n",
    "    Y_in = torch.cat((torch.ones(X.size(0), 1).long(), Y_in), dim=-1)[:, :-1]\n",
    "    \n",
    "    X = X.to(device)\n",
    "    xl = xl.to(device)\n",
    "    Y_in = Y_in.to(device)\n",
    "    yl = yl.to(device)\n",
    "    encoder_outputs, h = encoder.forward(X, xl)\n",
    "    decoder_outputs, h = decoder(Y_in, yl, h, encoder_outputs, xl)\n",
    "    return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "vocab_size = 32\n",
    "sequence_length = 30 \n",
    "\n",
    "embedding_dim = 8\n",
    "num_layers = 2\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3.4424784183502197\n",
      "10 3.3087024688720703\n",
      "15 2.843031406402588\n",
      "20 2.6411025524139404\n",
      "25 2.290308713912964\n",
      "30 2.102931261062622\n",
      "35 1.9856836795806885\n",
      "40 1.9223207235336304\n",
      "45 1.6761579513549805\n",
      "50 1.6298596858978271\n",
      "55 1.4496746063232422\n",
      "60 1.609636664390564\n",
      "65 1.5982531309127808\n",
      "70 1.6129547357559204\n",
      "75 1.438043236732483\n",
      "80 1.407720685005188\n",
      "85 1.4099887609481812\n",
      "90 1.363685965538025\n",
      "95 1.6918905973434448\n",
      "100 1.495357871055603\n",
      "105 1.2779377698898315\n",
      "110 1.344093680381775\n",
      "115 1.342970609664917\n",
      "120 1.3789728879928589\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1e662714d3c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device='cpu'\n",
    "\n",
    "encoder = S2SEncoder(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "decoder = S2SAttnDecoder(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "for i in range(10000):\n",
    "    if i and not i%5:\n",
    "        print(i, loss.item())\n",
    "    X, Y, xl, yl = dummy_batch(batch_size, vocab_size, sequence_length)\n",
    "    opt.zero_grad()\n",
    "    out = s2s(encoder, decoder, X, Y, xl, yl, device).transpose(2,1)\n",
    "    loss = crit(out, Y.to(device))\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
